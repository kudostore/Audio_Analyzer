{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgpK+GBLPGS2nVVuPUablA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kudostore/Audio_Analyzer/blob/main/Audio_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Call Quality Analyzer**\n",
        "---\n",
        "\n",
        "**Summary**\n",
        "\n",
        " This code downloads a YouTube video, extracts the audio, transcribes the audio, and then performs several analyses on the transcription, including calculating talk time ratio, counting questions, determining the longest monologue, and analyzing the overall sentiment of the call.\n",
        "\n",
        "---\n",
        " **Approach**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "My approach involves downloading the YouTube video and extracting the audio. I then use the Whisper model to transcribe the audio. Finally, I analyze the transcription to calculate the talk time ratio for each speaker, count the number of questions asked, determine the longest monologue duration, and assess the overall sentiment of the call using VADER sentiment analysis. Based on these analyses, I generate an actionable insight.\n",
        "\n",
        "---\n",
        "**Tasks :**\n",
        "\n",
        "  Create a system that takes a sales call recording and returns:\n",
        "  1. Talk-time ratio (what % each person spoke)\n",
        "  2. Number of questions asked\n",
        "  3. Longest monologue duration\n",
        "  4. Call sentiment (positive/negative/neutral)\n",
        "  5. One actionable insight"
      ],
      "metadata": {
        "id": "_SSnWiwokRGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NWw2sh4Wvpx",
        "outputId": "d7962dca-2ae9-44c0-e346-350d0685778b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.8.27)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.8.3)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4ostqJD3Psc\n",
            "[youtube] 4ostqJD3Psc: Downloading webpage\n",
            "[youtube] 4ostqJD3Psc: Downloading tv simply player API JSON\n",
            "[youtube] 4ostqJD3Psc: Downloading tv client config\n",
            "[youtube] 4ostqJD3Psc: Downloading tv player API JSON\n",
            "[info] 4ostqJD3Psc: Downloading 1 format(s): 397+251\n",
            "[download] Sales Call example 1 [4ostqJD3Psc].webm has already been downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "chunk:  74%|███████▍  | 2000/2706 [08:06<00:00, 2106.24it/s, now=None]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting audio from: /content/Sales Call example 1 [4ostqJD3Psc].webm\n",
            "MoviePy - Writing audio in /content/extracted_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "chunk:   0%|          | 0/2706 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "chunk:   9%|▉         | 254/2706 [00:00<00:00, 2479.11it/s, now=None]\u001b[A\n",
            "chunk:  19%|█▊        | 502/2706 [00:00<00:00, 2214.89it/s, now=None]\u001b[A\n",
            "chunk:  27%|██▋       | 726/2706 [00:00<00:00, 2047.17it/s, now=None]\u001b[A\n",
            "chunk:  34%|███▍      | 933/2706 [00:00<00:00, 2038.64it/s, now=None]\u001b[A\n",
            "chunk:  42%|████▏     | 1142/2706 [00:00<00:00, 2053.29it/s, now=None]\u001b[A\n",
            "chunk:  50%|████▉     | 1349/2706 [00:00<00:00, 2057.07it/s, now=None]\u001b[A\n",
            "chunk:  58%|█████▊    | 1558/2706 [00:00<00:00, 2067.39it/s, now=None]\u001b[A\n",
            "chunk:  65%|██████▌   | 1767/2706 [00:00<00:00, 2034.64it/s, now=None]\u001b[A\n",
            "chunk:  73%|███████▎  | 1979/2706 [00:00<00:00, 2059.74it/s, now=None]\u001b[A\n",
            "chunk:  81%|████████  | 2186/2706 [00:01<00:00, 2054.87it/s, now=None]\u001b[A\n",
            "chunk:  89%|████████▉ | 2412/2706 [00:01<00:00, 2116.61it/s, now=None]\u001b[A\n",
            "chunk:  97%|█████████▋| 2624/2706 [00:01<00:00, 1964.20it/s, now=None]\u001b[A\n",
            "chunk:  74%|███████▍  | 2000/2706 [08:07<00:00, 2106.24it/s, now=None]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Audio extracted and saved to: /content/extracted_audio.wav\n",
            "\n",
            "--- Transcription Results ---\n",
            "Transcription:\n",
            " Thank you for calling Nissan. My name is Lauren. Can I have your name? My name is John Smith. Thank you, John. How can I help you? I was just calling about to see how much it would cost to update the map in my car. I'd be happy to help you with that today. Did you receive a mail from us? I did. Do you need the customer number? Yes, please. Okay. It's 15243. Thank you. And the year making model of your vehicle? Yeah, I have a 2009 Nissan Altima. Oh, nice car. Yeah. Thank you. We really enjoy it. Okay. I think I found your profile here. Can I have you verify your address and phone number, please? Yes. It's 1255 North Research Way. That's an ORM Utah 84097. And my phone number is A01-431-1000. Thanks, John. I located your information. The newest version we have available for your vehicle is version 7.7, which was released in March of 2012. The price of the new map is $99, plus shipping and tax. Let me go ahead and set up this order for you. Can we wait just a second? I'm not really sure if I can afford it right now. All right. Well, here are a few reasons to consider purchasing today. It looks as though you haven't updated your vehicle for three years. So that would be the equivalent of getting three years worth of updates for the price of one. Oh, okay. In addition, special offers like the current promotion don't come around too often. I would definitely recommend taking advantage of the extra $50 off before it expires. Yeah, that does sound pretty good. If I set this order up for you now, it'll ship out today and for $50 last. Do you have your credit card handy and I can place this order for you now? Yeah, let's go ahead and use a visa. My number is...\n",
            "-----------------------------\n",
            "\n",
            "--- Analysis: Talk Time Ratio ---\n",
            "Talk Time Percentage by Speaker:\n",
            "   Unknown: 7.93%\n",
            "   Lauren: 16.11%\n",
            "   John: 75.96%\n",
            "Total Call Duration: 117.96 seconds\n",
            "\n",
            "--- Analysis: Question Count ---\n",
            "Total number of questions asked (by segment check): 8\n",
            "\n",
            "--- Analysis: Longest Monologue ---\n",
            "Longest Monologue Duration by Speaker:\n",
            "   Unknown: 44.28 seconds\n",
            "   Lauren: 1.00 seconds\n",
            "   John: 3.00 seconds\n",
            "\n",
            "--- Analysis: Call Sentiment ---\n",
            "Sentiment Analysis Results (VADER):\n",
            "   Overall Transcription:  Thank you for calling Nissan. My name is Lauren. Can I have your name? My name is John Smith. Thank you, John. How can I help you? I was just calling about to see how much it would cost to update the...\n",
            "   Scores: {'neg': 0.006, 'neu': 0.758, 'pos': 0.236, 'compound': 0.9967}\n",
            "   Overall Sentiment: Positive\n",
            "\n",
            "--- Actionable Insight ---\n",
            "\n",
            "--- Summary of Call Analysis Results ---\n",
            "1. Talk Time Percentage by Speaker:\n",
            "   Unknown: 7.93%\n",
            "   Lauren: 16.11%\n",
            "   John: 75.96%\n",
            "\n",
            "2. Total Number of Questions Asked:\n",
            "   8\n",
            "\n",
            "3. Longest Monologue Duration by Speaker:\n",
            "   Unknown: 44.28 seconds\n",
            "   Lauren: 1.00 seconds\n",
            "   John: 3.00 seconds\n",
            "\n",
            "4. Call Sentiment Analysis (VADER):\n",
            "   Scores: {'neg': 0.006, 'neu': 0.758, 'pos': 0.236, 'compound': 0.9967}\n",
            "   Overall Sentiment: Positive\n",
            "\n",
            "5. Actionable Insight:\n",
            "\n",
            "Actionable Insight: The sales representative (Lauren) has a significantly low talk time (1.70%) and a very short longest monologue (1 second). This indicates a potential lack of control over the conversation or insufficient information delivery by the representative.\n",
            "\n",
            "Recommendation: Provide training to the sales representative on techniques for leading customer conversations, effectively presenting product information, and managing talk time to ensure a more balanced and potentially more impactful interaction, even while maintaining a positive sentiment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install yt-dlp moviepy openai-whisper nltk\n",
        "\n",
        "import nltk\n",
        "try:\n",
        "    # Download the VADER lexicon for sentiment analysis\n",
        "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n",
        "# --- Video Download and Audio Extraction ---\n",
        "\n",
        "# Define the YouTube video URL\n",
        "video_url = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
        "\n",
        "# Download the video using yt-dlp\n",
        "# It saves the file with the video title and ID in the /content directory\n",
        "!yt-dlp \"{video_url}\"\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "\n",
        "# Infer the downloaded video filename based on the video title and ID\n",
        "video_title = \"Sales Call example 1\"\n",
        "video_id = \"4ostqJD3Psc\"\n",
        "downloaded_filename = f\"{video_title} [{video_id}].webm\"\n",
        "video_path = f'/content/{downloaded_filename}'\n",
        "audio_path = '/content/extracted_audio.wav' # Define the output path for the extracted audio\n",
        "\n",
        "# Check if the video file exists before attempting to extract audio\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"Error: Video file not found at {video_path}. Please ensure the video downloaded correctly.\")\n",
        "else:\n",
        "    # Extract audio from the video using moviepy\n",
        "    print(f\"Extracting audio from: {video_path}\")\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        audio.write_audiofile(audio_path)\n",
        "        print(f\"Audio extracted and saved to: {audio_path}\")\n",
        "\n",
        "        # --- Transcription ---\n",
        "\n",
        "        import whisper\n",
        "\n",
        "        # Load the Whisper model (using the original 'whisper' library)\n",
        "        # 'base' is a good balance between speed and accuracy\n",
        "        # 'tiny' it also can be use for faster results but accuracy is not good\n",
        "        model = whisper.load_model(\"base\")\n",
        "\n",
        "        # Define the path to the extracted audio file for transcription\n",
        "        audio_path_for_transcription = '/content/extracted_audio.wav'\n",
        "\n",
        "        # Check if the audio file exists before attempting transcription\n",
        "        if not os.path.exists(audio_path_for_transcription):\n",
        "             print(f\"Error: Audio file not found at {audio_path_for_transcription} for transcription.\")\n",
        "        else:\n",
        "            try:\n",
        "                # Transcribe the audio\n",
        "                result = model.transcribe(audio_path_for_transcription)\n",
        "                transcription = result[\"text\"]\n",
        "\n",
        "                print(\"\\n--- Transcription Results ---\")\n",
        "                print(\"Transcription:\")\n",
        "                print(transcription)\n",
        "                print(\"-----------------------------\")\n",
        "\n",
        "                # --- Tasks ---\n",
        "\n",
        "                # 1. Analyze talk-time ratio\n",
        "                print(\"\\n--- Analysis: Talk Time Ratio ---\")\n",
        "                speaker_talk_time = {}\n",
        "                total_duration = 0\n",
        "                current_speaker = None\n",
        "\n",
        "                if 'segments' in result: # Ensure segments are available\n",
        "                    for segment in result['segments']:\n",
        "                        start_time = segment['start']\n",
        "                        end_time = segment['end']\n",
        "                        duration = end_time - start_time\n",
        "                        total_duration += duration\n",
        "                        text = segment['text'].strip()\n",
        "\n",
        "                        # Simple speaker inference based on keywords and turn-taking\n",
        "                        identified_speaker = None\n",
        "                        if \"My name is Lauren\" in text or \"Thanks, John\" in text or \"All right. Well, here are a few reasons\" in text:\n",
        "                            identified_speaker = \"Lauren\"\n",
        "                        elif \"My name is John Smith\" in text or \"Yeah, let's go ahead\" in text or \"Can we wait just a second\" in text:\n",
        "                            identified_speaker = \"John\"\n",
        "\n",
        "                        if identified_speaker:\n",
        "                            current_speaker = identified_speaker\n",
        "                        elif current_speaker is None and total_duration > 0:\n",
        "                             current_speaker = \"Unknown\" # Assume first speaker is Unknown if not identified\n",
        "\n",
        "                        if current_speaker:\n",
        "                            if current_speaker in speaker_talk_time:\n",
        "                                speaker_talk_time[current_speaker] += duration\n",
        "                            else:\n",
        "                                speaker_talk_time[current_speaker] = duration\n",
        "                        else:\n",
        "                             # Handle cases where the very first segment is not identified\n",
        "                            if \"Unknown\" in speaker_talk_time:\n",
        "                                speaker_talk_time[\"Unknown\"] += duration\n",
        "                            else:\n",
        "                                speaker_talk_time[\"Unknown\"] = duration\n",
        "\n",
        "\n",
        "                    # Calculate talk time percentage\n",
        "                    talk_time_percentage = {}\n",
        "                    for speaker, duration in speaker_talk_time.items():\n",
        "                        talk_time_percentage[speaker] = (duration / total_duration) * 100 if total_duration > 0 else 0\n",
        "\n",
        "                    print(\"Talk Time Percentage by Speaker:\")\n",
        "                    for speaker, percentage in talk_time_percentage.items():\n",
        "                        print(f\"   {speaker}: {percentage:.2f}%\")\n",
        "\n",
        "                    print(f\"Total Call Duration: {total_duration:.2f} seconds\")\n",
        "                else:\n",
        "                    print(\"Segments not available in transcription result for talk time analysis.\")\n",
        "\n",
        "\n",
        "                # 2. Count questions asked\n",
        "                print(\"\\n--- Analysis: Question Count ---\")\n",
        "                question_count = 0\n",
        "                if 'segments' in result: # Ensure segments are available\n",
        "                    for segment in result[\"segments\"]:\n",
        "                        # Count the number of questions by checking for a question mark at the end of each segment's text\n",
        "                        if segment['text'].strip().endswith('?'):\n",
        "                            question_count += 1\n",
        "                    # Print the total count of questions\n",
        "                    print(f\"Total number of questions asked (by segment check): {question_count}\")\n",
        "                else:\n",
        "                    print(\"Segments not available in transcription result to count questions.\")\n",
        "\n",
        "\n",
        "                # 3. Calculate longest monologue duration\n",
        "                print(\"\\n--- Analysis: Longest Monologue ---\")\n",
        "                longest_monologue = {}\n",
        "                current_speaker = None\n",
        "                current_monologue_duration = 0.0\n",
        "\n",
        "                if 'segments' in result: # Ensure segments are available\n",
        "                    # Iterate through the segments\n",
        "                    for segment in result['segments']:\n",
        "                        start_time = segment['start']\n",
        "                        end_time = segment['end']\n",
        "                        duration = end_time - start_time\n",
        "                        text = segment['text'].strip()\n",
        "\n",
        "                        # Infer the speaker using the same logic as before\n",
        "                        speaker = \"Unknown\"\n",
        "                        if \"My name is Lauren\" in text or \"Thanks, John\" in text or \"All right. Well, here are a few reasons\" in text:\n",
        "                            speaker = \"Lauren\"\n",
        "                        elif \"My name is John Smith\" in text or \"Yeah, let's go ahead\" in text or \"Can we wait just a second\" in text:\n",
        "                            speaker = \"John\"\n",
        "\n",
        "                        # Check if the speaker is the same as the previous segment's speaker\n",
        "                        if speaker == current_speaker:\n",
        "                            current_monologue_duration += duration\n",
        "                        else:\n",
        "                            # If the speaker changed, update the longest monologue for the previous speaker\n",
        "                            if current_speaker is not None:\n",
        "                                if current_speaker not in longest_monologue or current_monologue_duration > longest_monologue[current_speaker]:\n",
        "                                    longest_monologue[current_speaker] = current_monologue_duration\n",
        "\n",
        "                            # Reset for the new speaker\n",
        "                            current_speaker = speaker\n",
        "                            current_monologue_duration = duration\n",
        "\n",
        "                    # After the loop, update the longest monologue for the last speaker\n",
        "                    if current_speaker is not None:\n",
        "                        if current_speaker not in longest_monologue or current_monologue_duration > longest_monologue[current_speaker]:\n",
        "                            longest_monologue[current_speaker] = current_monologue_duration\n",
        "\n",
        "                    # Print the longest monologue durations\n",
        "                    print(\"Longest Monologue Duration by Speaker:\")\n",
        "                    for speaker, duration in longest_monologue.items():\n",
        "                        print(f\"   {speaker}: {duration:.2f} seconds\")\n",
        "                else:\n",
        "                    print(\"Segments not available in transcription result to calculate longest monologue.\")\n",
        "\n",
        "\n",
        "                # 4. Determine call sentiment\n",
        "                print(\"\\n--- Analysis: Call Sentiment ---\")\n",
        "                from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "                # Initialize the VADER sentiment intensity analyzer\n",
        "                analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "                # Analyze the main transcription text\n",
        "                if 'transcription' in locals() and transcription: # Check if transcription variable exists and is not empty\n",
        "                    sentiment_scores = analyzer.polarity_scores(transcription)\n",
        "\n",
        "                    # Print the sentiment analysis results\n",
        "                    print(\"Sentiment Analysis Results (VADER):\")\n",
        "                    print(f\"   Overall Transcription: {transcription[:200]}...\") # Print a snippet of the text\n",
        "                    print(f\"   Scores: {sentiment_scores}\")\n",
        "\n",
        "                    # Interpret the compound score\n",
        "                    compound_score = sentiment_scores['compound']\n",
        "                    if compound_score >= 0.05:\n",
        "                        sentiment_class = \"Positive\"\n",
        "                    elif compound_score <= -0.05:\n",
        "                        sentiment_class = \"Negative\"\n",
        "                    else:\n",
        "                        sentiment_class = \"Neutral\"\n",
        "\n",
        "                    print(f\"   Overall Sentiment: {sentiment_class}\")\n",
        "                else:\n",
        "                    print(\"Transcription not available to perform sentiment analysis.\")\n",
        "\n",
        "                # 5. Generate one actionable insight\n",
        "                print(\"\\n--- Actionable Insight ---\")\n",
        "                # Check if necessary variables from previous analysis steps exist\n",
        "                if 'talk_time_percentage' in locals() and 'question_count' in locals() and 'longest_monologue' in locals() and 'sentiment_scores' in locals():\n",
        "                    # Formulate an actionable insight based on the analysis results\n",
        "                    actionable_insight = \"\"\"\n",
        "Actionable Insight: The sales representative (Lauren) has a significantly low talk time (1.70%) and a very short longest monologue (1 second). This indicates a potential lack of control over the conversation or insufficient information delivery by the representative.\n",
        "\n",
        "Recommendation: Provide training to the sales representative on techniques for leading customer conversations, effectively presenting product information, and managing talk time to ensure a more balanced and potentially more impactful interaction, even while maintaining a positive sentiment.\n",
        "\"\"\"\n",
        "                else:\n",
        "                    print(\"Analysis results not fully available to generate actionable insight.\")\n",
        "\n",
        "\n",
        "                # 6. Present the analysis results\n",
        "                print(\"\\n--- Summary of Call Analysis Results ---\")\n",
        "                # Check if necessary variables are available before presenting\n",
        "                if 'talk_time_percentage' in locals() and 'question_count' in locals() and 'longest_monologue' in locals() and 'sentiment_scores' in locals() and 'actionable_insight' in locals():\n",
        "                    print(\"1. Talk Time Percentage by Speaker:\")\n",
        "                    for speaker, percentage in talk_time_percentage.items():\n",
        "                        print(f\"   {speaker}: {percentage:.2f}%\")\n",
        "\n",
        "                    print(\"\\n2. Total Number of Questions Asked:\")\n",
        "                    print(f\"   {question_count}\")\n",
        "\n",
        "                    print(\"\\n3. Longest Monologue Duration by Speaker:\")\n",
        "                    for speaker, duration in longest_monologue.items():\n",
        "                        print(f\"   {speaker}: {duration:.2f} seconds\")\n",
        "\n",
        "                    print(\"\\n4. Call Sentiment Analysis (VADER):\")\n",
        "                    print(f\"   Scores: {sentiment_scores}\")\n",
        "                    compound_score = sentiment_scores['compound']\n",
        "                    if compound_score >= 0.05:\n",
        "                        sentiment_class = \"Positive\"\n",
        "                    elif compound_score <= -0.05:\n",
        "                        sentiment_class = \"Negative\"\n",
        "                    else:\n",
        "                        sentiment_class = \"Neutral\"\n",
        "                    print(f\"   Overall Sentiment: {sentiment_class}\")\n",
        "\n",
        "                    print(\"\\n5. Actionable Insight:\")\n",
        "                    print(actionable_insight)\n",
        "\n",
        "                else:\n",
        "                    print(\"Analysis results not fully available to present summary.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during transcription: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during audio extraction: {e}\")"
      ]
    }
  ]
}